from dataclasses import dataclass
from typing import List, Dict, Tuple
import datetime

from peewee import fn
from tqdm import tqdm

import check_bingji_shuangpin
import check_lu_shuangpin
import check_xhe_shuangpin
import check_zrm_shuangpin
from common import InputSchema, XHE_SP_SCHEMA, get_char_to_xhe_phones, LU_SP_SCHEMA, get_char_to_lu_phones, \
    ZRM_SP_SCHEMA, get_char_to_zrm_phones, BINGJI_SP_SCHEMA, get_char_to_bingji_phones, get_char_to_xhe_shapes, \
    is_all_alpha, SchemaConfig, PINYIN_SCHEMA, ShapeSchema, XHE_SHAPE_SCHAME, ZRM_SHAPE_SCHEMA, get_char_to_zrm_shapes, \
    LU_SHAPE_SCHEMA, get_char_to_lu_shapes
from tables import CharPhoneTable, WordPhoneTable, EngWordTable, SimplerTable, TangshiTable, TwoStrokesWordsTable


@dataclass
class EncodeDecode(object):
    encode: str
    decode: str
    weight: float
    shape_size: int = -1


def generate_one_hit_char() -> List[EncodeDecode]:
    items = [
        "å»\tq",
        "æˆ‘\tw",
        "äºŒ\te",
        "äºº\tr",
        "ä»–\tt",
        "ä¸€\ty",
        "æ˜¯\tu",
        "å‡º\ti",
        "å“¦\to",
        "å¹³\tp",
        "å•Š\ta",
        "ä¸‰\ts",
        "çš„\td",
        "é\tf",
        "ä¸ª\tg",
        "å’Œ\th",
        "å°±\tj",
        "å¯\tk",
        "äº†\tl",
        "åœ¨\tz",
        "å°\tx",
        "æ‰\tc",
        "è¿™\tv",
        "ä¸\tb",
        "ä½ \tn",
        "æ²¡\tm",
    ]
    return [EncodeDecode(encode=e.split("\t")[1], decode=e.split("\t")[0], weight=100000000, shape_size=0) for e in items]


def generate_tow_hits_char(schema: InputSchema) -> List[EncodeDecode]:
    chars = """å» æˆ‘ äºŒ äºº ä»–  ä¸€ æ˜¯ å‡º å“¦ å¹³ å•Š ä¸‰ çš„ é ä¸ª å’Œ å°± å¯ äº† å° æ‰ è¿™ ä¸ ä½  æ²¡ é˜¿ çˆ± å®‰ æ˜‚ å¥¥ æŠŠ ç™½ åŠ å¸® æŠ¥ è¢« æœ¬ è¹¦ æ¯” è¾¹ è¡¨ åˆ« æ»¨ å¹¶ æ‹¨ éƒ¨ æ“¦ èœ å‚ è— è— è‰ æµ‹ å²‘ æ›¾ æ‹† äº§ è¶… è½¦ é™ˆ æˆ åƒ å†² æŠ½ å¤„ æ£ ä¼  çª— å¹ çº¯ æˆ³ æ¬¡ å·® ä» å‡‘ ç²— çªœ å‚¬ æ‘ é”™ å¤§ ä»£ ä½† å½“ åˆ° å¾— å¾— ç­‰ åœ° ç‚¹ è·Œ å®š ä¸¢ åŠ¨ éƒ½ è¯» æ®µ å¯¹ é¡¿ å¤š é¢ æ¬¸ æ© å—¯ è€Œ æ³• å æ”¾ è´¹ åˆ† é£ ä½› å¦ å‰¯ å˜ è¯¥ å¹² åˆš é«˜ å„ ç»™ è·Ÿ æ›´ å…± å¤Ÿ å¤ æŒ‚ æ€ª å…³ å…‰ è´µ æ»š è¿‡ å“ˆ è¿˜ å« è¡Œ å¥½ ä½• é»‘ å¾ˆ æ¨ª çº¢ å æˆ· è¯ å æ¢ é»„ ä¼š æ·· æˆ– å‡  åŠ  é—´ å°† å« æ¥ è¿› ç» ä¹… æ® å· å‡ å¡ å¼€ çœ‹ æŠ— é  å…‹ å‰‹ è‚¯ å‘ ç©º å£ é…· å¤¸ å¿« å®½ å†µ äº å›° æ‰© æ‹‰ æ¥ æµª è€ æœˆ ä¹ ç±» å†· é‡Œ è¿ ä¸¤ æ–™ åˆ— æ— å¦ åˆ˜ é¾™ æ¥¼ è·¯ ä¹± è®º è½ ç‡ å— ä¹° æ…¢ å¿™ æ¯› ä¹ˆ æ¯ ä»¬ æ¢¦ ç±³ é¢ ç§’ ç­ æ°‘ å æœ« æŸ ç›® é‚£ éš¾ å›Š é—¹ å‘¢ å†… å«© èƒ½ æ³¥ å¹´ é¸Ÿ æ æ‚¨ å® ç‰› å¼„ æ€’ æš– è™ æŒª å¥³ æ¬§ æ€• æ’ ç›˜ æ— è·‘ é… ç›† ç¢° æ‰¹ ç‰‡ ç¥¨ æ’‡ å“ å‡­ ç ´ å‰– æ™® å…¶ æ° å‰ å¼º æ¡¥ ä¸” è¯· äº² ç©· æ±‚ åŒº å…¨ å´ ç¾¤ ç„¶ è®© ç»• çƒ­ ä»» ä» æ—¥ å®¹ è‚‰ å¦‚ è½¯ è‹¥ æ’’ èµ› æ•£ æ‰« è‰² æ£® åƒ§ å•¥ æ™’ å±± ä¸Š å°‘ è®¾ æ·± ç”Ÿ æ—¶ å— å¸… æ‹´ åŒ æ°´ é¡º å›› é€ æœ è‹ ç®— å² æ‰€ å¥¹ å¤ª è°ˆ æ±¤ å¥— ç‰¹ ç–¼ ä½“ å¤© è°ƒ è´´ å¬ åŒ å¤´ å›¾ å›¢ æ¨ æ‰˜ æŒ– å¤– å®Œ ç‹ ä¸º é—® ç¿ å–” æ¡ æ—  ç³» ä¸‹ å…ˆ æƒ³ ç¬‘ äº› æ–° ç†Š ä¿® éœ€ é€‰ å­¦ äºš çœ¼ æ · è¦ ä¹Ÿ ä»¥ å›  åº” å“Ÿ ç”¨ æœ‰ ä¸ å…ƒ äº‘ å’‹ å† æ—© åˆ™ è´¼ æ€ å¢ æ‰ å  é•¿ é•¿ æ‰¾ ç€ çœŸ æ­£ åª ä¸­ å‘¨ ä¸» æŠ“ æ‹½ è½¬ è£… è¿½ æ¡Œ å­— æ€» èµ° ç»„ æœ€ åš"""
    if schema == XHE_SP_SCHEMA:
        char_to_phones = get_char_to_xhe_phones()
    elif schema == LU_SP_SCHEMA:
        char_to_phones = get_char_to_lu_phones()
    elif schema == ZRM_SP_SCHEMA:
        char_to_phones = get_char_to_zrm_phones()
    elif schema == BINGJI_SP_SCHEMA:
        char_to_phones = get_char_to_bingji_phones()
    else:
        raise RuntimeError(f"{schema} not found")

    exists_chars = set()
    items: List[EncodeDecode] = []
    for char in filter(lambda e: e != "", chars.strip().split(" ")):
        if char in exists_chars:
            continue
        exists_chars.add(char)
        if char not in char_to_phones:
            print(f"A: {char} has no phones.")
            continue
        for phone in char_to_phones[char]:
            items.append(EncodeDecode(encode=phone, decode=char, weight=10000000, shape_size=0))

    return items

def generate_two_strokes_words() -> List[EncodeDecode]:
    result = []
    for item in TwoStrokesWordsTable.select().where(TwoStrokesWordsTable.is_first == True).order_by(TwoStrokesWordsTable.id.asc()):
        result.append(EncodeDecode(encode=item.encode, decode=item.word, weight=10000000, shape_size=0))
    return result

def get_dd_cmds():
    cmds = [
        '$ddcmd(<date.z> <time.hh>æ—¶<time.mm>åˆ†)\touj',
        '$ddcmd(run(calc.exe),[è®¡ç®—å™¨])\tojsq',
        '$ddcmd(<last.1>,â˜…)\tz',
        '$ddcmd(run(%apppath%\\),[å®‰è£…ç›®å½•])\toav',
        '$ddcmd(<date.yyyy>å¹´<date.m>æœˆ<date.d>æ—¥,<date.yyyy>å¹´<date.m>æœˆ<date.d>æ—¥)\torq',
        '$ddcmd(<date.YYYY>å¹´<date.M>æœˆ<date.D>æ—¥,<date.YYYY>å¹´<date.M>æœˆ<date.D>æ—¥)\torq',
        '$ddcmd(<date.yyyy>-<date.mm>-<date.dd>,<date.yyyy>-<date.mm>-<date.dd>)\torq',
        '$ddcmd(<date.z> <time.h>:<time.mm>,<date.z> <time.h>:<time.mm>)\touj',
        '$ddcmd(run(https://www.baidu.com/s?wd=<last.0>),[ç™¾åº¦]ï¼š<last.0>)\toss',
        '$ddcmd(run(https://www.zdic.net/hans/?q=<last.1>),[æ±‰å…¸]ï¼š<last.1>)\tohd',
        '$ddcmd(run(http://www.xhup.club/?search_word=<last.1>),[å°é¹¤æŸ¥å½¢]ï¼š<last.1>)\tohd',
        '$ddcmd(run(cmd.exe),[å‘½ä»¤æç¤ºè¡Œ])\tocm',
        '$ddcmd(run(::{20D04FE0-3AEA-1069-A2D8-08002B30309D}),[æˆ‘çš„ç”µè„‘])\todn',
        '$ddcmd(run(control.exe),[æ§åˆ¶é¢æ¿])\tokv',
        '$ddcmd(run(::{450D8FBA-AD25-11D0-98A8-0800361B1103}),[æˆ‘çš„æ–‡æ¡£])\towd',
        '$ddcmd(run(winword.exe),[word])\towd',
        '$ddcmd(run(excel.exe),[excel])\toec',
        '$ddcmd(run(mspaint.exe),[ç”»å›¾])\toht',
        '$ddcmd(run(notepad.exe),[è®°äº‹æœ¬])\toju',
        '$ddcmd(keyboard(<32+Alt><78>),[æœ€å°åŒ–])\tozx',
        '$ddcmd(keyboard(<83+Shift+Win>),[æˆªå±])\tojp',
        '$ddcmd(keyboard(<68+Win>),[æ¡Œé¢])\tovm',
        '$ddcmd(help(keyboardmap.html,600,500),[é”®ç›˜å›¾])\tojp',
        '$ddcmd(config(/do anjianshezhi),[æŒ‰é”®å®šä¹‰])\toaj',
        '$ddcmd(config(/do å¸¸ç”¨),[å¸¸ç”¨é¡¹])\toiy',
        '$ddcmd(config(/do ç•Œé¢),[ç•Œé¢é¡¹])\tojm',
        '$ddcmd(config(/do ç è¡¨),[ç è¡¨é¡¹])\tomb',
        '$ddcmd(config(/do é«˜çº§),[é«˜çº§é¡¹])\togj',
        '$ddcmd(config(/do gaojishezhi),[é«˜çº§è®¾ç½®])\togj',
        '$ddcmd(config(/do about),[å…³äºé¡¹])\togy',
        '$ddcmd(set([-IMEè®¾ç½®-],åµŒå…¥æ–‡æœ¬å†…å®¹=åµŒå…¥ç¼–ç ),[åµŒå…¥ç¼–ç ])\toiq',
        '$ddcmd(set([-IMEè®¾ç½®-],åµŒå…¥æ–‡æœ¬å†…å®¹=åµŒå…¥é¦–é€‰),[åµŒå…¥é¦–é€‰])\toiq',
        '$ddcmd(set([-IMEè®¾ç½®-],åµŒå…¥æ–‡æœ¬å†…å®¹=ä¼ ç»Ÿæ ·å¼),[ä¼ ç»Ÿæ ·å¼])\toiq',
        '$ddcmd(set([-IMEè®¾ç½®-],éšè—å€™é€‰çª—å£=åˆ‡æ¢),[æ˜¾éšå€™é€‰çª—])\toic',
        '$ddcmd(set([-SKINè®¾ç½®-],å€™é€‰åˆ—è¡¨æ’åˆ—æ ·å¼=æ¨ªæ’),[æ¨ªæ’çª—å£])\toih',
        '$ddcmd(set([-SKINè®¾ç½®-],å€™é€‰åˆ—è¡¨æ’åˆ—æ ·å¼=ç«–æ’),[ç«–æ’çª—å£])\toih',
        '$ddcmd(set([-DMEè®¾ç½®-],æŸ¥è¯¢è¾“å…¥åªæŸ¥å•å­—=åˆ‡æ¢),[ä¸‡èƒ½é”®æŸ¥å­—è¯åˆ‡æ¢])\toiz',
        '$ddcmd(config(/do è¾“å‡ºåæŸ¥),[åæŸ¥]ï¼š<last.1>)\tofi',
        '$ddcmd(config(/do å‰ªè´´æ¿åæŸ¥),[å‰ªè´´æ¿åæŸ¥])\tofi',
        '$ddcmd(config(/do åœ¨çº¿åŠ è¯),[åœ¨çº¿åŠ è¯])\tojc',
        '$ddcmd(convert(ä¸­è‹±æ–‡æ ‡ç‚¹,åˆ‡æ¢),[ä¸­è‹±æ–‡æ ‡ç‚¹åˆ‡æ¢])\tovy',
        '$ddcmd(convert(å…¨åŠè§’,åˆ‡æ¢),[å…¨åŠè§’åˆ‡æ¢])\toqb',
        '$ddcmd(convert(ç¹ä½“è¾“å‡º,åˆ‡æ¢),[ç®€ç¹åˆ‡æ¢])\tojf',
        '$ddcmd(keyboard(<35><36+Shift><46>),[åˆ å½“å‰è¡Œ])\toui',
        '$ddcmd(keyboard(<35><36+Shift><46>),[åˆ å½“å‰è¡Œ])\toiu',
        '$ddcmd(config(/dict <last.1>),[å­—å…¸]ï¼š<last.1>)\tozd',
        '$ddcmd(set([-IMEè®¾ç½®-],ç¦ç”¨é¼ æ ‡æ‚¬åœè¯å…¸=åˆ‡æ¢),[å€™é€‰çª—å­—å…¸å¼€å…³])\tozd',
        '$ddcmd(set([-IMEè®¾ç½®-],æ£€ç´¢æ¬¡æ˜¾ç è¡¨=æ˜¯),[å¯ç”¨å•å­—å…¨ç ])\toqm',
        '$ddcmd(set([-IMEè®¾ç½®-],æ£€ç´¢æ¬¡æ˜¾ç è¡¨=å¦),[å…³é—­])\toqm',
        '$ddcmd(set([-IMEè®¾ç½®-],è¾“å…¥æ–¹æ¡ˆ=ä¸»+è¾…),[å¯ç”¨è¯è¯­è¾…åŠ©])\tocf',
        '$ddcmd(set([-IMEè®¾ç½®-],è¾“å…¥æ–¹æ¡ˆ=ä¸»),[å…³é—­])\tocf',
        '$ddcmd(set([-SKINè®¾ç½®-],ä½¿ç”¨çš®è‚¤åç§°=åˆ‡æ¢),[æ¢è‚¤])\tohf',
        '$ddcmd(set([-SKINè®¾ç½®-],ä½¿ç”¨çš®è‚¤åç§°=fw.col),[é»˜è®¤])\tohf',
        '$ddcmd(keyboard(<173>),[é™éŸ³å¼€å…³])\tojy',
        '$ddcmd(run(https://flypy.com),[å°é¹¤å®˜ç½‘])\txhgw',
        '$ddcmd(run(https://bbs.flypy.com),[å°é¹¤è®ºå›])\txhlt',
        '$ddcmd(run(http://flypy.ys168.com),[å°é¹¤ç½‘ç›˜])\txhwp',
    ]
    return cmds


def generate_single_chars(schema: InputSchema, shape_schema: ShapeSchema) -> List[EncodeDecode]:
    if shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schema}")

    result: List[EncodeDecode] = []
    for item in CharPhoneTable.select().order_by(
            CharPhoneTable.priority.desc(), CharPhoneTable.id.asc()):
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        elif schema == PINYIN_SCHEMA:
            phones = item.full
        else:
            raise RuntimeError(f"schema not found {schema}")

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if schema != PINYIN_SCHEMA and item.char in char_to_shape:
            used_shapes = set()
            for shape in char_to_shape[item.char]:
                if shape in used_shapes:
                    continue
                used_shapes.add(shape)
                result.append(EncodeDecode(decode=item.char, encode=phones + shape, weight=item.priority, shape_size=len(shape)))
        else:
            if schema != PINYIN_SCHEMA:
                print(f"æ²¡æœ‰å½¢ç çš„å­—ï¼š{item.char}")
            result.append(EncodeDecode(decode=item.char, encode=phones, weight=item.priority, shape_size=0))
    return result


def generate_simpler_words(char_threshold: int, word_threshold: int, schema: InputSchema, shape_schema: ShapeSchema) -> Tuple[
    List[EncodeDecode], List[EncodeDecode]]:
    if shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schema}")

    single_chars: Dict[str, CharPhoneTable] = {}
    for item in CharPhoneTable.select().order_by(
            CharPhoneTable.priority.desc(), CharPhoneTable.id.asc()):
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        else:
            raise RuntimeError(f"schema not found {schema}")

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.char in char_to_shape:
            used_shapes = set()
            for shape in char_to_shape[item.char]:
                if shape in used_shapes:
                    continue
                used_shapes.add(shape)
                single_chars[phones + shape] = item
        else:
            print(f"{item}, have no shapes")
            single_chars[phones] = item

    high_pri_simpler_words: List[EncodeDecode] = []
    low_pri_simpler_words: List[EncodeDecode] = []
    exit_word_phones = set()
    for item in WordPhoneTable.select().order_by(
            fn.LENGTH(WordPhoneTable.word),
            WordPhoneTable.priority.desc(), WordPhoneTable.id.asc()):
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        else:
            raise RuntimeError(f"schema not found {schema}")

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.word + ":" + phones in exit_word_phones:
            continue
        exit_word_phones.add(item.word + ":" + phones)
        if len(item.word) > 2:
            continue
        if phones in single_chars:
            if item.priority >= word_threshold and single_chars[phones].priority < char_threshold:
                high_pri_simpler_words.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
            else:
                low_pri_simpler_words.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
    return high_pri_simpler_words, low_pri_simpler_words


def generate_full_words(
        schema: InputSchema, 
        shape_schema: ShapeSchema, 
        is_ff: bool) -> List[EncodeDecode]:
    """
    ç”Ÿæˆå…¨ç è¯
    :param schema: InputSchema, è¾“å…¥æ–¹æ¡ˆ
    :param shape_schema: ShapeSchema, å½¢ç æ–¹æ¡ˆ
    :param is_ff: bool, æ˜¯å¦é¦–å°¾ç›¸åŒ
    :return: List[EncodeDecode]
    """
    if shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schema}")

    result: List[EncodeDecode] = []
    exit_word_phones = set()
    for item in WordPhoneTable.select().order_by(
            fn.LENGTH(WordPhoneTable.word),
            WordPhoneTable.priority.desc(), WordPhoneTable.id.asc()):
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        elif schema == PINYIN_SCHEMA:
            phones = item.full.replace(" ", "")
        else:
            raise RuntimeError(f"schema not found {schema}")

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.word + ":" + phones in exit_word_phones:
            print(f"é‡å¤çš„è®°å½•: {item}")
            continue
        exit_word_phones.add(item.word + ":" + phones)
        if schema != PINYIN_SCHEMA and item.word[0] in char_to_shape and item.word[-1] in char_to_shape:
            used_shapes = set()
            for shape_first in char_to_shape[item.word[0]]:
                for shape_last in char_to_shape[item.word[-1]]:
                    shapes = [
                        shape_first[0] + shape_last[0] if is_ff else shape_last[-1],
                        # shape_first[0] + shape_last[-1],
                    ]
                    for shape in shapes:
                        if shape in used_shapes:
                            continue
                        used_shapes.add(shape)
                        encode = phones + shape
                        decode = item.word
                        result.append(EncodeDecode(encode=encode, decode=decode, weight=item.priority, shape_size=len(shape)))
        else:
            if schema != PINYIN_SCHEMA:
                print(f"æ²¡æœ‰å½¢ç çš„è¯ï¼š{item.word}")
            result.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
            continue

    return result


def generate_eng() -> List[str]:
    result = []
    for local_item in EngWordTable.select().where(
            EngWordTable.priority > 0).order_by(fn.LENGTH(EngWordTable.word), EngWordTable.priority):
        if not is_all_alpha(local_item.word):
            continue
        result.append(f"{local_item.word}\t{local_item.word}")
    return result


def generate_simpler() -> List[EncodeDecode]:
    result = []
    for item in SimplerTable.select().where(
            SimplerTable.priority > 0).order_by(SimplerTable.priority.desc()):
        encode = item.keys
        decode = item.words
        rule = f"{decode}\t{encode}"
        result.append(EncodeDecode(encode=encode, decode=decode, weight=item.priority, shape_size=0))

    return result


def generate_schema(config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f"# å°é¹­éŸ³å½¢æ–¹æ¡ˆ\n")
        fout.write(f"# encoding: utf-8\n")
        fout.write(f"# æœºå™¨ç”Ÿæˆï¼Œè¯·å‹¿ä¿®æ”¹\n")
        fout.write(f"\n")

        fout.write(f"\nschema:\n")
        fout.write(f"  schema_id: {config.schema_id}\n")
        fout.write(f"  name: ğŸ¦©{config.name}\n")
        fout.write(f'  version: "{config.version}"\n')
        fout.write(f'  author: \n')
        for author in config.authors:
            fout.write(f'    - {author} \n')
        fout.write(f'  description: |\n')
        fout.write(f'     {config.description}\n')

        fout.write(f"\nswitches:\n")
        fout.write(f"  - name: ascii_mode \n")
        fout.write(f"    reset: 0\n")
        fout.write(f"  - name: full_shape\n")
        fout.write(f"  - name: zh_simp\n")
        fout.write(f"    reset: 1\n")
        fout.write(f"    states: [ ç¹, ç®€ ]\n")
        fout.write(f"  - name: ascii_punct\n")
        fout.write(f"    reset: 0\n")

        fout.write(f"\nengine:\n")
        fout.write(f"  processors:\n")
        fout.write(f"    - ascii_composer\n")
        fout.write(f"    - recognizer\n")
        fout.write(f"    - key_binder\n")
        fout.write(f"    - speller\n")
        fout.write(f"    - punctuator\n")
        fout.write(f"    - selector\n")
        fout.write(f"    - navigator\n")
        fout.write(f"    - express_editor\n")
        fout.write(f"  segmentors:\n")
        fout.write(f"    - ascii_segmentor\n")
        fout.write(f"    - matcher\n")
        fout.write(f"    - abc_segmentor\n")
        fout.write(f"    - punct_segmentor\n")
        fout.write(f"    - fallback_segmentor\n")
        fout.write(f"  translators:\n")
        fout.write(f"    - punct_translator\n")
        fout.write(f"    - table_translator\n")
        if len(config.reverse_dict) != 0:
            fout.write(f"    - reverse_lookup_translator\n")
        fout.write(f"  filters:\n")
        fout.write(f"    - simplifier\n")
        fout.write(f"    - uniquifier\n")

        fout.write("\n")

        fout.write(f"speller:\n")
        fout.write(f"  alphabet: 'zyxwvutsrqponmlkjihgfedcba'\n")
        fout.write(f"  initials: 'abcdefghijklmnopqrstuvwxyz'\n")
        fout.write(f"  auto_select: true\n")
        fout.write(f"  auto_select_pattern: {config.auto_select_pattern}\n")

        fout.write(f"\n")

        fout.write(f"translator:\n")
        fout.write(f"  dictionary: {config.schema_id}\n")
        fout.write(f"  enable_charset_filter: false\n")
        fout.write(f"  enable_sentence: false\n")
        fout.write(f"  enable_completion: true\n")
        fout.write(f"  enable_user_dict: true\n")
        fout.write(f"  enable_encoder: true\n")
        fout.write(f"  encode_commit_history: true\n")
        fout.write(f"  max_phrase_length: 3\n")

        fout.write(f"\n\n")

        fout.write(f"punctuator:\n")
        fout.write(f"  import_preset: default\n")

        fout.write(f"\n")

        if len(config.reverse_dict) != 0:
            fout.write(f"reverse_lookup:\n")
            fout.write(f"  dictionary: {config.reverse_dict}\n")
            fout.write(f"  prefix: \"`\"\n")
            fout.write(f"  tips: [æ‹¼éŸ³]\n")

        fout.write(f"\n")

        fout.write(f"key_binder:\n")
        fout.write(f"  import_preset: default\n")
        fout.write(f"  bindings:\n")
        fout.write(f"    - {{accept: comma, send: comma, when: paging}} #æ³¨é”€é€—å·ç¿»é¡µ\n")
        fout.write(f"    - {{accept: period, send: period, when: has_menu}} #æ³¨é”€å¥å·ç¿»é¡µ\n")
        fout.write(f"    - {{accept: semicolon, send: 2, when: has_menu}} #åˆ†å·æ¬¡é€‰\n")
        fout.write(f"    - {{accept: apostrophe, send: 3, when: has_menu}} #å•å¼•å·3é€‰\n")
        fout.write(f"    - {{accept: bracketleft, send: 4, when: has_menu}} #å•å¼•å·4é€‰\n")
        fout.write(f"    - {{accept: bracketright, send: 5, when: has_menu}} #å•å¼•å·5é€‰\n")
        fout.write(f"    - {{accept: dollar, send: 2, when: composing}}\n")
        fout.write(f"    - {{accept: Release+dollar, send: period, when: composing}}\n")
        fout.write(f"    - {{accept: Release+period, send: period, when: composing}}\n")
        fout.write(f"    - {{accept: bar, send: 2, when: composing}}\n")
        fout.write(f"    - {{accept: Release+bar, send: comma, when: composing}}\n")
        fout.write(f"    - {{accept: Release+comma, send: comma, when: composing}}\n")
        fout.write(f'    - {{accept: "Tab", send: Page_Down, when: has_menu}}\n')
        fout.write(f'    - {{accept: "Tab", send: Escape, when: composing}}\n')
        fout.write(f'    - {{accept: "Caps_Lock", send: Escape, when: composing}}\n')
        fout.write(f'    - {{accept: "Shift_R", send: Escape, when: composing}}\n')
        fout.write(f'    - {{accept: "Shift+space", toggle: full_shape, when: always}} #åˆ‡æ¢å…¨åŠè§’\n')
        fout.write(f'    - {{accept: "Control+0", toggle: ascii_punct, when: always}}\n')
        fout.write(f'    - {{when: composing, accept: space, send: Escape}}\n')
        fout.write(f'    - {{when: has_menu, accept: space, send: space}}\n')

        fout.write(f"\n")

        if len(config.reverse_dict) != 0:
            fout.write(f"recognizer:\n")
            fout.write(f"  import_preset: default\n")
            fout.write(f"  patterns:\n")
            fout.write(f'    reverse_lookup: "^`[a-z]*\'?$"\n')

        fout.write(f"\n")

        fout.write(f"menu:\n")
        fout.write(f"  page_size: 5\n")

        fout.write(f"style:\n")
        fout.write(f"  horizontal: true\n")


def generate_shuangpin_dict(schema_config: SchemaConfig, outpath: str):
    if schema_config.input_schema == XHE_SP_SCHEMA:
        char_to_phones = get_char_to_xhe_phones()
    elif schema_config.input_schema == LU_SP_SCHEMA:
        char_to_phones = get_char_to_lu_phones()
    elif schema_config.input_schema == ZRM_SP_SCHEMA:
        char_to_phones = get_char_to_zrm_phones()
    elif schema_config.input_schema == BINGJI_SP_SCHEMA:
        char_to_phones = get_char_to_bingji_phones()
    else:
        raise RuntimeError(f"{schema_config.input_schema} not found")

    print(f"total {len(char_to_phones)} char phones")

    if schema_config.shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif schema_config.shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif schema_config.shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"{schema_config.shape_schema} not found")

    print(f"total {len(char_to_shape)} char shapes")

    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f"# {schema_config.schema_id} dictionary\n")
        fout.write(f"# encoding: utf-8\n")
        fout.write(f"# \n")
        fout.write(f"# {schema_config.name}ç è¡¨\n")
        fout.write(f"# æœºå™¨ç”Ÿæˆï¼Œè¯·å‹¿ä¿®æ”¹\n")

        fout.write(f"\n---\n")
        fout.write(f"name: {schema_config.schema_id}\n")
        fout.write(f'version: "{schema_config.version}"\n')
        fout.write(f'sort: original\n')
        fout.write(f'use_preset_vocabulary: false\n')  # æ˜¯å¦ä½¿ç”¨é¢„è®¾è¯è¡¨

        fout.write(f'columns:\n')
        fout.write(f'  - text\n')
        fout.write(f'  - code\n')

        fout.write(f'encoder:\n')
        fout.write(f'  rules:\n')
        fout.write(f'    - length_equal: 2\n')
        fout.write(f'      formula: "AaAbBaBbAcBc"\n')
        fout.write(f'    - length_equal: 3\n')
        fout.write(f'      formula: "AaAbBaBbCaCbAcCc"\n')
        fout.write(f'    - length_equal: 4\n')
        fout.write(f'      formula: "AaAbBaBbCaCbDaDbAcDc"\n')
        fout.write(f'    - length_equal: 5\n')
        fout.write(f'      formula: "AaAbBaBbCaCbDaDbEaEbAcEc"\n')
        fout.write(f'    - length_equal: 6\n')
        fout.write(f'      formula: "AaAbBaBbCaCbDaDbEaEbFaFbAcFc"\n')
        fout.write(f'    - length_equal: 7\n')
        fout.write(f'      formula: "AaAbBaBbCaCbDaDbEaEbFaFbGaGbAcGc"\n')
        fout.write(f'    - length_equal: 8\n')
        fout.write(f'      formula: "AaAbBaBbCaCbDaDbEaEbFaFbGaGbHaHbAcHc"\n')

        fout.write(f"...\n")

        fout.write(f"\n# å•å­—\n")

        one_hit_chars = generate_one_hit_char()
        with tqdm(total=len(one_hit_chars), desc="å†™å…¥ä¸€ç®€ç ") as pbar:
            for item in one_hit_chars:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        two_hits_chars = generate_tow_hits_char(schema_config.input_schema)
        with tqdm(total=len(two_hits_chars), desc="å†™å…¥äºŒç®€ç ") as pbar:
            for item in two_hits_chars:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        two_strokes_words = generate_two_strokes_words()
        with tqdm(total=len(two_strokes_words), desc="å†™å…¥äºŒç¬”è¯") as pbar:
            for item in two_strokes_words:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        single_chars = generate_single_chars(schema_config.input_schema, schema_config.shape_schema)
        with tqdm(total=len(single_chars), desc="å†™å…¥ç®€ç å•å­—") as pbar:
            for item in single_chars:
                if item.shape_size == 2:
                    fout.write(f"{item.decode}\t{item.encode[:-1]}\n")
                pbar.update()

        special_words = set()
        high_word, low_words = generate_simpler_words(100, 2000, schema_config.input_schema, schema_config.shape_schema)
        with tqdm(total=len(high_word), desc="å†™å…¥é«˜é¢‘è¯ç®€ç ") as pbar:
            for item in high_word:
                special_words.add(item.decode)
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        with tqdm(total=len(single_chars), desc="å†™å…¥å…¨ç å•å­—") as pbar:
            for item in single_chars:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        with tqdm(total=len(low_words), desc="å†™å…¥ä½é¢‘è¯ç®€ç ") as pbar:
            for item in low_words:
                special_words.add(item.decode)
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        fout.write(f"\n# è¯è¯­\n")

        full_words = generate_full_words(schema_config.input_schema, schema_config.shape_schema, schema_config.is_ff)
        with tqdm(total=len(full_words), desc="å†™å…¥è¯") as pbar:
            for item in full_words:

                if item.decode not in special_words:
                    if item.shape_size >= 2:
                        fout.write(f"{item.decode}\t{item.encode[0:-2]}\n")
                if item.shape_size >= 1: 
                    fout.write(f"{item.decode}\t{item.encode[0:-1]}\n")
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        tangshi_words = generate_tangshi_words(schema_config.input_schema, schema_config.shape_schema, schema_config.is_ff)
        with tqdm(total=len(tangshi_words), desc="å†™å…¥è¯—è¯") as pbar:
            for item in tangshi_words:
                if item.shape_size >= 2:
                    fout.write(f"{item.decode}\t{item.encode[0:-2]}\n")
                if item.shape_size >= 1:
                    fout.write(f"{item.decode}\t{item.encode[0:-1]}\n")
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        len4_words = generate_4_len_word_simpler_items(schema_config.input_schema, schema_config.shape_schema, schema_config.is_ff)
        with tqdm(total=len(len4_words), desc="å†™å…¥è¯ç®€ç ") as pbar:
            for item in len4_words:
                if item.shape_size >= 2:
                    fout.write(f"{item.decode}\t{item.encode[0:-2]}\n")
                if item.shape_size >= 1:
                    fout.write(f"{item.decode}\t{item.encode[0:-1]}\n")
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        simpler_words = generate_simpler()
        with tqdm(total=len(simpler_words), desc="å†™å…¥ç®€è¯") as pbar:
            for item in simpler_words:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()


def generate_pinyin_dict(schema_config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f"# {schema_config.schema_id} dictionary\n")
        fout.write(f"# encoding: utf-8\n")
        fout.write(f"# \n")
        fout.write(f"# {schema_config.name}ç è¡¨\n")
        fout.write(f"# æœºå™¨ç”Ÿæˆï¼Œè¯·å‹¿ä¿®æ”¹\n")

        fout.write(f"\n---\n")
        fout.write(f"name: {schema_config.schema_id}\n")
        fout.write(f'version: "{schema_config.version}"\n')
        fout.write(f'sort: original\n')
        fout.write(f'use_preset_vocabulary: false\n')  # æ˜¯å¦ä½¿ç”¨é¢„è®¾è¯è¡¨

        fout.write(f'columns:\n')
        fout.write(f'  - text\n')
        fout.write(f'  - code\n')

        fout.write(f"...\n")

        fout.write(f"\n# å•å­—\n")

        single_chars = generate_single_chars(PINYIN_SCHEMA, XHE_SHAPE_SCHAME)

        special_words = set()

        with tqdm(total=len(single_chars), desc="å†™å…¥å…¨ç å•å­—") as pbar:
            for item in single_chars:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()

        fout.write(f"\n# è¯è¯­\n")

        full_words = generate_full_words(PINYIN_SCHEMA, XHE_SHAPE_SCHAME, True)
        with tqdm(total=len(full_words), desc="å†™å…¥è¯") as pbar:
            for item in full_words:
                fout.write(f"{item.decode}\t{item.encode}\n")
                pbar.update()


def generate_schema_custom(config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f"# {config.schema_id} custom settings\n")
        fout.write(f"# encoding: utf-8\n")
        fout.write(f"# \n")
        fout.write(f"# {config.name} custom settings\n")
        fout.write(f"# æœºå™¨ç”Ÿæˆï¼Œè¯·å‹¿ä¿®æ”¹\n")

        fout.write(f"patch:\n")
        fout.write(f"  punctuator/half_shape:\n")
        fout.write(f"    '/': 'ã€'\n")
        fout.write(f"    '<': 'ã€Š'\n")
        fout.write(f"    '>': 'ã€‹'\n")
        fout.write(f"    '`': 'Â·'\n")

        fout.write(f'\n')

        fout.write(f'  "ascii_composer/switch_key/Shift_L": commit_code\n')
        fout.write(f'  "ascii_composer/switch_key/Shift_R": commit_code\n')

        fout.write(f'\n')

        fout.write(f'  "style/display_tray_icon": true\n')
        fout.write(f'  "style/horizontal": true\n')  # æ¨ªæ’æ˜¾ç¤º
        fout.write(f'  "style/font_face": "Microsoft YaHei"\n')  # å­—ä½“
        fout.write(f'  "style/font_point": 12\n')  # å­—ä½“å¤§å°
        fout.write(f'  "style/inline_preedit": true\n')  # åµŒå…¥å¼å€™é€‰çª—å•è¡Œæ˜¾ç¤º
        fout.write(f'  "style/layout/border_width": 0\n')
        fout.write(f'  "style/layout/border": 0\n')
        fout.write(f'  "style/layout/margin_x": 7\n')  # å€™é€‰å­—å·¦å³è¾¹è·
        fout.write(f'  "style/layout/margin_y": 7\n')  # å€™é€‰å­—ä¸Šä¸‹è¾¹è·
        fout.write(f'  "style/layout/hilite_padding": 8\n')  # å€™é€‰å­—èƒŒæ™¯è‰²è‰²å—é«˜åº¦ è‹¥æƒ³å€™é€‰å­—èƒŒæ™¯è‰²å—æ— è¾¹ç•Œå¡«å……å€™é€‰æ¡†ï¼Œä»…éœ€å…¶é«˜åº¦å’Œå€™é€‰å­—ä¸Šä¸‹è¾¹è·ä¸€è‡´å³å¯
        fout.write(f'  "style/layout/hilite_spacing": 1\n')  # åºå·å’Œå€™é€‰å­—ä¹‹é—´çš„é—´éš”
        fout.write(f'  "style/layout/spacing": 7\n')  # ä½œç”¨ä¸æ˜
        fout.write(f'  "style/layout/candidate_spacing": 8\n')  # å€™é€‰å­—é—´éš”
        fout.write(f'  "style/layout/round_corner": 5\n')  # å€™é€‰å­—èƒŒæ™¯è‰²å—åœ†è§’å¹…åº¦


def generate_weasel_custom(config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f'customization:\n')
        fout.write(f'  distribution_code_name: Weasel\n')
        fout.write(f'  distribution_version: 0.14.3\n')
        fout.write(f'  generator: "Rime::SwitcherSettings"\n')
        fout.write(f'  modified_time: "Mon Jun  6 16:28:09 2022"\n')
        fout.write(f'  rime_version: 1.5.3\n')

        fout.write(f'\n')

        fout.write(f'patch:\n')
        fout.write(f'  style: \n')
        fout.write(f'    color_scheme: LuColor\n')
        fout.write(f'  preset_color_schemes: \n')
        fout.write(f'    LuColor:\n')
        fout.write(f'      name: LuColor\n')
        fout.write(f'      author: "ledao"\n')
        fout.write(f'      alpha: 1.0\n')
        fout.write(f'      border_height: 5\n')
        fout.write(f'      border_width: 0\n')
        fout.write(f'      border_color: 0xffffff\n')  # å€™é€‰æ¡† è¾¹æ¡†é¢œè‰²
        fout.write(f'      back_color: 0xF4F4F6\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      label_color: 0xaaaaaa\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      font_point: 18\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      corner_radius: 5\n')
        fout.write(f'      candidate_format: "%c %@ "\n')
        fout.write(f"      horizontal: true\n")
        fout.write(f"      line_spacing: 5\n")
        fout.write(f"      base_offset: 0\n")
        fout.write(f"      preedit_back_color: 0x364572\n")
        fout.write(f'      hilited_corner_radius: 5\n')
        fout.write(f'      hilited_candidate_text_color: 0x000000\n')  # å€™é€‰å­—é¢œè‰²
        fout.write(f'      hilited_candidate_back_color: 0xE6E6E6\n')  # å€™é€‰å­—èƒŒæ™¯è‰²
        fout.write(f'      hilited_candidate_label_color: 0x88000000\n')
        fout.write(f'      hilited_comment_text_color: 0xF19C38\n')
        fout.write(f'      candidate_text_color: 0x222222\n')  # æœªå€™é€‰å­—é¢œchè‰²
        fout.write(f'      comment_text_color: 0x5AC461\n')
        fout.write(f'      comment_font_point: 14\n')
        fout.write(f'      inline_preedit: true\n')
        fout.write(f'      spacing: 5\n')
        fout.write(f'      hilited_text_color: 0x8E8E93\n')  # å·²é€‰æ‹©å­—å³ä¾§æ‹¼éŸ³ æ–‡å­—é¢œè‰²
        fout.write(f'      hilited_back_color: 0xEFEFF4\n')  # å·²é€‰æ‹©å­—å³ä¾§æ‹¼éŸ³ èƒŒæ™¯è‰²
        fout.write(f'\n\n')


def generate_squirrel_custom(config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f'patch:\n')
        fout.write(f'  style: \n')
        fout.write(f'    color_scheme: LuColor\n')
        fout.write(f'  preset_color_schemes: \n')
        fout.write(f'    LuColor:\n')
        fout.write(f'      name: LuColor\n')
        fout.write(f'      author: "ledao"\n')
        fout.write(f'      alpha: 1.0\n')
        fout.write(f'      border_height: 5\n')
        fout.write(f'      border_width: 0\n')
        fout.write(f'      border_color: 0xffffff\n')  # å€™é€‰æ¡† è¾¹æ¡†é¢œè‰²
        fout.write(f'      back_color: 0xF4F4F6\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      label_color: 0xaaaaaa\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      font_point: 18\n')  # å€™é€‰æ¡† èƒŒæ™¯è‰²
        fout.write(f'      corner_radius: 5\n')
        fout.write(f'      candidate_format: "%c %@ "\n')
        fout.write(f"      horizontal: true\n")
        fout.write(f"      line_spacing: 5\n")
        fout.write(f"      base_offset: 0\n")
        fout.write(f"      preedit_back_color: 0x364572\n")
        fout.write(f'      hilited_corner_radius: 5\n')
        fout.write(f'      hilited_candidate_text_color: 0x000000\n')  # å€™é€‰å­—é¢œè‰²
        fout.write(f'      hilited_candidate_back_color: 0xE6E6E6\n')  # å€™é€‰å­—èƒŒæ™¯è‰²
        fout.write(f'      hilited_candidate_label_color: 0x88000000\n')
        fout.write(f'      hilited_comment_text_color: 0xF19C38\n')
        fout.write(f'      candidate_text_color: 0x222222\n')  # æœªå€™é€‰å­—é¢œchè‰²
        fout.write(f'      comment_text_color: 0x5AC461\n')
        fout.write(f'      comment_font_face: PingFang SC\n')
        fout.write(f'      comment_font_point: 14\n')
        fout.write(f'      inline_preedit: true\n')
        fout.write(f'      spacing: 5\n')
        fout.write(f'      hilited_text_color: 0x8E8E93\n')  # å·²é€‰æ‹©å­—å³ä¾§æ‹¼éŸ³ æ–‡å­—é¢œè‰²
        fout.write(f'      hilited_back_color: 0xEFEFF4\n')  # å·²é€‰æ‹©å­—å³ä¾§æ‹¼éŸ³ èƒŒæ™¯è‰²
        fout.write(f'\n\n')
        fout.write(f'  app_options:\n')
        fout.write(f'    com.termius-dmg.mac: \n')
        fout.write(f'      ascii_mode: true\n')


def generate_default_custom(config: SchemaConfig, outpath: str):
    with open(outpath, 'w', encoding='utf8') as fout:
        fout.write(f'patch:\n')
        fout.write(f'  schema_list: \n')
        fout.write(f'    - {{schema: {config.schema_id}}}\n')
        fout.write(f'    - {{schema: xiaolu_fuzhu_pinyin}}\n')

        fout.write(f'  "switcher/hotkeys": \n')
        fout.write(f'    - "Control+Alt+grave"\n')
        fout.write(f'    - "F4"\n')

def generate_dd(schema: InputSchema, output_dir: str, shape_schema: ShapeSchema, check_db: bool, is_ff: bool):
    if check_db:
        if schema == XHE_SP_SCHEMA:
            check_xhe_shuangpin.main()
        elif schema == LU_SP_SCHEMA:
            check_lu_shuangpin.main()
        elif schema == ZRM_SP_SCHEMA:
            check_zrm_shuangpin.main()
        elif schema == BINGJI_SP_SCHEMA:
            check_bingji_shuangpin.main()
        else:
            raise RuntimeError(f"{schema} not found")

    sys_top_chars_data = f"{output_dir}/sys_top_chars_data.txt"
    with open(sys_top_chars_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -1\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç®€ç å•å­—\n")
        one_hit_chars = generate_one_hit_char()
        two_hits_chars = generate_tow_hits_char(schema)
        with tqdm(total=len(one_hit_chars) + len(two_hits_chars), desc="å†™å…¥ç®€ç å•å­—") as pbar:
            for item in one_hit_chars:
                fout.write(f"{item.decode}\t{item.encode}#åº{90000}\n")
            pbar.update(len(one_hit_chars))
            for item in two_hits_chars:
                fout.write(f"{item.decode}\t{item.encode}#åº{80000}\n")
            pbar.update(len(two_hits_chars))

        # äºŒç®€è¯æ”¾è¿™é‡Œ
        two_strokes_words = generate_two_strokes_words()
        for item in two_strokes_words:
            fout.write(f"{item.decode}\t{item.encode}#åº{79000}\n")

        single_chars = generate_single_chars(schema, shape_schema)
        for item in single_chars:
            fout.write(f"{item.decode}\t{item.encode[:-1]}#åº{78000}\n")

    high_freq_words, low_freq_words = generate_simpler_words(100, 2000, schema, shape_schema)
    sys_high_freq_word_data = f"{output_dir}/sys_high_word_data.txt"
    with open(sys_high_freq_word_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -3\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=é«˜é¢‘ç®€è¯\n")

        for item in high_freq_words:
            fout.write(f"{item.decode}\t{item.encode}#åº{75000}\n")

    sys_single_char_data = f"{output_dir}/sys_single_char_data.txt"
    with open(sys_single_char_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -2\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿå•å­—\n")
        for item in single_chars:
            fout.write(f"{item.decode}\t{item.encode}#åº{70000}\n")

    sys_low_freq_word_data = f"{output_dir}/sys_low_word_data.txt"
    with open(sys_low_freq_word_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -4\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ä½é¢‘ç®€è¯\n")
        for item in low_freq_words:
            fout.write(f"{item.decode}\t{item.encode}#åº{65000}\n")

    sys_word_data = f"{output_dir}/sys_word_data.txt"
    with open(sys_word_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -5\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿè¯ç»„\n")
        for item in generate_full_words(schema, shape_schema, is_ff):
            fout.write(f"{item.decode}\t{item.encode}#åº{60000}\n")

    sys_simpler_word_data = f"{output_dir}/sys_simpler_word_data.txt"
    with open(sys_simpler_word_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -6\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿç®€è¯\n")
        for item in generate_4_len_word_simpler_items(schema, shape_schema, is_ff):
            fout.write(f"{item.decode}\t{item.encode}#åº{55000}\n")

    sys_tangshi_data = f"{output_dir}/sys_tangshi_word_data.txt"
    with open(sys_tangshi_data, 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -7\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿå”è¯—\n")
        for item in generate_tangshi_words(schema, shape_schema, is_ff):
            fout.write(f"{item.decode}\t{item.encode}#åº{52000}\n")

    with open(f'{output_dir}/sys_eng_data.txt', 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -8\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿè‹±æ–‡\n")
        for item in generate_eng():
            fout.write(f"{item}#åº{50000}\n")

    with open(f'{output_dir}/sys_simpler_data.txt', 'w',
              encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -9\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç³»ç»Ÿç®€ç \n")
        for item in generate_simpler():
            fout.write(f"{item.decode}\t{item.encode}#åº{40000}\n")

    with open(f'{output_dir}/sys_cmd_data.txt', 'w', encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=ä¸»ç -10\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=ç›´é€šè½¦\n")
        cmds = get_dd_cmds()
        for cmd in cmds:
            fout.write(f"{cmd}\n")

    with open(f'{output_dir}/sys_fuzhu_pinyin_data.txt', 'w',
              encoding='utf8') as fout:
        fout.write("---config@ç è¡¨åˆ†ç±»=è¾…ç ç è¡¨\n")
        fout.write("---config@å…è®¸ç¼–è¾‘=æ˜¯\n")
        fout.write(f"---config@ç è¡¨åˆ«å=è¾…åŠ©æ‹¼éŸ³\n")
        for item in generate_single_chars(PINYIN_SCHEMA, XHE_SHAPE_SCHAME): # shape_schema éšæ„
            fout.write(f"{item.decode}\t{item.encode}#åº{30000}\n")
        for item in generate_full_words(PINYIN_SCHEMA, XHE_SHAPE_SCHAME, is_ff): # shape_schema éšæ„
            fout.write(f"{item.decode}\t{item.encode}#åº{30000}\n")


def generate_rime(schema_config: SchemaConfig, output_dir: str):
    if schema_config.check_db:
        if schema_config.input_schema == XHE_SP_SCHEMA:
            check_xhe_shuangpin.main()
        elif schema_config.input_schema == LU_SP_SCHEMA:
            check_lu_shuangpin.main()
        elif schema_config.input_schema == ZRM_SP_SCHEMA:
            check_zrm_shuangpin.main()
        elif schema_config.input_schema == BINGJI_SP_SCHEMA:
            check_bingji_shuangpin.main()
        else:
            raise RuntimeError(f"{schema_config.input_schema} not found")

    generate_schema(schema_config, output_dir + f"/{schema_config.schema_id}.schema.yaml")
    generate_schema_custom(schema_config, output_dir + f"/{schema_config.schema_id}.custom.yaml")
    if schema_config.input_schema == PINYIN_SCHEMA:
        generate_pinyin_dict(schema_config, output_dir + f"/{schema_config.schema_id}.dict.yaml")
    else:
        generate_shuangpin_dict(schema_config, output_dir + f"/{schema_config.schema_id}.dict.yaml")
        generate_weasel_custom(schema_config, output_dir + f"/weasel.custom.yaml")
        generate_squirrel_custom(schema_config, output_dir + f"/squirrel.custom.yaml")
        generate_default_custom(schema_config, output_dir + f"/default.custom.yaml")



def generate_shouxin(schema: InputSchema, output_dir: str, shape_schema: ShapeSchema, check_db: bool, is_ff: bool):
    if check_db:
        if schema == XHE_SP_SCHEMA:
            check_xhe_shuangpin.main()
        elif schema == LU_SP_SCHEMA:
            check_lu_shuangpin.main()
        elif schema == ZRM_SP_SCHEMA:
            check_zrm_shuangpin.main()
        elif schema == BINGJI_SP_SCHEMA:
            check_bingji_shuangpin.main()
        else:
            raise RuntimeError(f"{schema} not found")
    version = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
    name = "xiaolu_"
    if schema == XHE_SP_SCHEMA:
        name += "xiaohe_shuangpin"
    elif schema == LU_SP_SCHEMA:
        name += "xiaolu_shuangpin"
    elif schema == ZRM_SP_SCHEMA:
        name += "ziranma_shuangpin"
    else:
        raise RuntimeError(f"{schema} not found")
    
    if shape_schema == XHE_SHAPE_SCHAME:
        name += "_xiaohe_xing"
    elif shape_schema == LU_SHAPE_SCHEMA:
        name += "_xiaolu_xing"
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        name += "_ziranma_xing"
    else:
        raise RuntimeError(f"{shape_schema} not found")

    duanyu_txt = f"{output_dir}/{name}_duanyu.txt"
    with open(duanyu_txt, 'w', encoding='utf8') as fout:
        fout.write(f";å°é¹­éŸ³å½¢ç³»åˆ—\n")
        fout.write(f";ç»´æŠ¤è€…: ledao, 790717479@qq.com\n")
        fout.write(f";ç‰ˆæœ¬ï¼š{version}\n")

        one_hit_chars = generate_one_hit_char()
        for item in one_hit_chars:
            fout.write(f"{item.encode}={item.decode}\n")

        two_hits_chars = generate_tow_hits_char(schema)
        for item in two_hits_chars:
            fout.write(f"{item.encode}={item.decode}\n")


        single_chars = generate_single_chars(schema, shape_schema)
        for item in single_chars:
            fout.write(f"{item.encode[:-1]}={item.decode}\n")

        high_freq_word, low_freq_words = generate_simpler_words(100, 2000, schema, shape_schema)
        for item in high_freq_word:
            fout.write(f"{item.encode}={item.decode}\n")

        for item in single_chars:
            fout.write(f"{item.encode}={item.decode}\n")

        for item in low_freq_words:
            fout.write(f"{item.encode}={item.decode}\n")

        for item in generate_full_words(schema, shape_schema, is_ff):
            fout.write(f"{item.encode}={item.decode}\n")

        two_strokes_words = generate_two_strokes_words()
        for item in two_strokes_words:
            fout.write(f"{item.encode}={item.decode}\n")
    
    fuzhuma_txt = f"{output_dir}/{name}_fuzhuma.txt"
    with open(fuzhuma_txt, "w", encoding='utf8') as fout:
        fout.write(f";å°é¹­éŸ³å½¢ç³»åˆ—\n")
        fout.write(f";ç»´æŠ¤è€…: ledao, 790717479@qq.com\n")
        fout.write(f";ç‰ˆæœ¬ï¼š{version}\n")
        for item in generate_single_chars(schema, shape_schema):
            fout.write(f"{item.decode}={' '.join(item.encode[-2:])} {item.encode[-2:]}\n")


def generate_4_len_wordphonetable_words(schema: InputSchema, shape_schema:ShapeSchema, is_ff: bool) -> List[EncodeDecode]:
    if shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schema}")

    result: List[EncodeDecode] = []
    exit_word_phones = set()
    for item in WordPhoneTable.select().order_by(
            fn.LENGTH(WordPhoneTable.word),
            WordPhoneTable.priority.desc(), WordPhoneTable.id.asc()):
        if len(item.word) <= 3:
            continue
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        else:
            raise RuntimeError(f"schema not found {schema}")

        simpler_phones = ""
        for i in range(len(phones)):
            if i % 2 == 0:
                simpler_phones += phones[i]
        phones = simpler_phones

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.word + ":" + phones in exit_word_phones:
            print(f"é‡å¤çš„è®°å½•: {item}")
            continue
        exit_word_phones.add(item.word + ":" + phones)
        if item.word[0] in char_to_shape and item.word[-1] in char_to_shape:
            used_shapes = set()
            for shape_first in char_to_shape[item.word[0]]:
                for shape_last in char_to_shape[item.word[-1]]:
                    shapes = [
                        shape_first[0] + shape_last[0] if is_ff else shape_last[-1],
                        # shape_first[0] + shape_last[-1],
                    ]
                    for shape in shapes:
                        if shape in used_shapes:
                            continue
                        used_shapes.add(shape)
                        encode = phones + shape
                        decode = item.word
                        result.append(EncodeDecode(encode=encode, decode=decode, weight=item.priority, shape_size=len(shape)))
        else:
            print(f"æ²¡æœ‰å½¢ç çš„è¯ï¼š{item.word}")
            result.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
            continue

    return result


def generate_4_len_tangshi_words(schema: InputSchema, shape_schame:ShapeSchema, is_ff: bool) -> List[EncodeDecode]:
    if shape_schame == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schame == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schame == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schame}")

    result: List[EncodeDecode] = []
    exit_word_phones = set()
    for item in TangshiTable.select().order_by(
            fn.LENGTH(TangshiTable.word),
            TangshiTable.priority.desc(), TangshiTable.id.asc()):
        if len(item.word) <= 3:
            continue
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        else:
            raise RuntimeError(f"schema not found {schema}")

        simpler_phones = ""
        for i in range(len(phones)):
            if i % 2 == 0:
                simpler_phones += phones[i]
        phones = simpler_phones

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.word + ":" + phones in exit_word_phones:
            print(f"é‡å¤çš„è®°å½•: {item}")
            continue
        exit_word_phones.add(item.word + ":" + phones)
        if item.word[0] in char_to_shape and item.word[-1] in char_to_shape:
            used_shapes = set()
            for shape_first in char_to_shape[item.word[0]]:
                for shape_last in char_to_shape[item.word[-1]]:
                    shapes = [
                        shape_first[0] + shape_last[0] if is_ff else shape_last[-1],
                        # shape_first[0] + shape_last[-1],
                    ]
                    for shape in shapes:
                        if shape in used_shapes:
                            continue
                        used_shapes.add(shape)
                        encode = phones + shape
                        decode = item.word
                        result.append(EncodeDecode(encode=encode, decode=decode, weight=item.priority, shape_size=len(shape)))
        else:
            print(f"æ²¡æœ‰å½¢ç çš„è¯ï¼š{item.word}")
            result.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
            continue

    return result


def generate_4_len_word_simpler_items(schema: InputSchema, shape_schema:ShapeSchema, is_ff) -> List[EncodeDecode]:
    result = []
    result.extend(generate_4_len_tangshi_words(schema, shape_schema, is_ff))
    return result


def generate_tangshi_words(schema: InputSchema, shape_schema:ShapeSchema, is_ff: bool) -> List[EncodeDecode]:
    if shape_schema == XHE_SHAPE_SCHAME:
        char_to_shape = get_char_to_xhe_shapes()
    elif shape_schema == ZRM_SHAPE_SCHEMA:
        char_to_shape = get_char_to_zrm_shapes()
    elif shape_schema == LU_SHAPE_SCHEMA:
        char_to_shape = get_char_to_lu_shapes()
    else:
        raise RuntimeError(f"shape_schema not found {shape_schema}")

    result: List[EncodeDecode] = []
    exit_word_phones = set()
    for item in TangshiTable.select().order_by(
            fn.LENGTH(TangshiTable.word),
            TangshiTable.priority.desc(), TangshiTable.id.asc()):
        if schema == XHE_SP_SCHEMA:
            phones = item.xhe
        elif schema == LU_SP_SCHEMA:
            phones = item.lu
        elif schema == ZRM_SP_SCHEMA:
            phones = item.zrm
        elif schema == BINGJI_SP_SCHEMA:
            phones = item.bingji
        else:
            raise RuntimeError(f"schema not found {schema}")

        if phones == "":
            raise RuntimeError(f"empty phones, {item}")

        if item.word + ":" + phones in exit_word_phones:
            print(f"é‡å¤çš„è®°å½•: {item}")
            continue
        exit_word_phones.add(item.word + ":" + phones)

        if item.word[0] in char_to_shape and item.word[-1] in char_to_shape:
            used_shapes = set()
            for shape_first in char_to_shape[item.word[0]]:
                for shape_last in char_to_shape[item.word[-1]]:
                    shapes = [
                        shape_first[0] + shape_last[0] if is_ff else shape_last[-1],
                        # shape_first[0] + shape_last[-1],
                    ]
                    for shape in shapes:
                        if shape in used_shapes:
                            continue
                        used_shapes.add(shape)
                        encode = phones + shape
                        decode = item.word
                        result.append(EncodeDecode(encode=encode, decode=decode, weight=item.priority, shape_size=len(shape)))
        else:
            print(f"æ²¡æœ‰å½¢ç çš„è¯ï¼š{item.word}")
            result.append(EncodeDecode(encode=phones, decode=item.word, weight=item.priority, shape_size=0))
            continue

    return result
